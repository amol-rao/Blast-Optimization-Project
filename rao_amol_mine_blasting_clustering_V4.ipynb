{"cells":[{"cell_type":"markdown","source":["# Investigating Mine Blasting Data Using Clustering Techniques\n<strong>Project Plan v1.0</strong><br/>\nDate: May 1, 2017<br/>\nVersion: V4<br/>\nAuthor: Amol Rao<br/>\nStudent Number: 994610453<br/>\nCourse: MIE1512"],"metadata":{}},{"cell_type":"markdown","source":["## Contents \nThe contents of this notebook are as follows:\n\n### 1. Introduction\n\n### 2. Project Plan & Task Breakdown\n\n### 3. Data Understanding\n\n### 4. Data Preparation\n\n### 5. Modeling \n\n### 6. Alternative Iterations\n\n### 7. Conclusions & Recommendations\n\n### 8. Bibliography"],"metadata":{}},{"cell_type":"markdown","source":["## 1.0) Introduction\n\n### 1.1) Motivation of the Problem\n\nThe motivation for this investigation is to better understand the factors driving the relationship between blast outcomes and rock mass parameters for an open pit gold mine located in North Eastern Ontario. \n\nThe blasting of rock at mines serves two primary purposes: a) to enable access to the mineral deposit containing the ore and b) fragment the ore to a suitable size for digging and processing. \n\nBoth objectives require optimization of the blasting process to minimize cost and maximize speed at which fragmented rock can be moved on for processing through improved digability. The efficiency of the blasting operation depends on a few key parameters, including; rock characteristics, properties and quantities of explosives, blast geometry, blast size, priming method and initiation sequence. \n\nAs part of this analysis, <strong> the geotechnical and blasting parameters </strong> will be explored to better understand groupings within the data. It is hoped the analysis can be used to eventually optimize blasting to reduce movement and make the fragmentation process more efficient. \n\n### 1.1) Analysis Method (Selected from Bibliography)\n\nData clustering refers to the unsupervised classification of features into patterns that then form meaningful groups. Data within 1 cluster are more similar than data in another cluster. As described by Jain et. al [8] there are four main components to a clustering task: \n\n* Pattern representation - Comprised of selection of the features and extraction for the dataset. This step also involves identifying number of classes in the representation.\n\n* Definition of pattern proximity - This is the distance function defined on the pairs of patterns. For example Euclidean distance is a common measure.\n\n* Clustering or grouping - There are many types of clustering techniques.  \n\n* Data abstraction - Once clustering has been done, a person with knowledge of the domain needs to identify what is represented and ensure the clusters are simple and compact for scalability. \n\n* Assessment of output - Perform some type of validity analysis on the cluster. \n\nHudaverdi [1] uses a hierarchical clustering technique to group 88 blast data into two groups of similarity. Blast design parameters such as burden, spacing, bench height and stemming as well as geotechnical data are used to to obtain two seperate clusters of data. Following this, each cluster is fed into a regression model to predict ground vibration, as indicated by a measure of the Peak Particle Velocity (PPV). A hierarchical technique is used to cluster all blasts into one of two groups. After standardization of the data, the Pearson coefficient of similarity is used to group data. \n\n### 1.3) Application to This Dataset\nDue to the constraint of using the MlLib toolkit - the clustering algorithm that will be explored in this analysis is the K-Means method. K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The number of clusters is optimized by reducing the error on the within set sum of squared error. \n\nThe items to be clustered are <strong> individual blasts </strong> based on the features to be described later on this notebook. The blast data  was recorded with instrumentation and manually and provided by Professor Kamran Esmaeili for this investigation."],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"plan\"></a>\n## 2.0) Project Plan & Task Breakdown\n\n<br/>\n### 2.1) Project Plan\n\nBased on the CRISP-DM method, these are the proposed tasks for the project:\n\n* <strong>Week 1 - Data Understanding</strong>\n  * Data Review \n  * Data verification (with mining professors)\n  * Exploratory analysis\n  * Review + identify potential analytics techniques\n \n* <strong>Week 2 - Data Preparation</strong>\n  * Data Cleaning\n  * Feature extraction and selection\n  * Integrate and format data for model\n  * Finalize modeling techniques to be used\n  \n* <strong>Week 3 - Modeling</strong>\n  * Implement selected modeling technique\n  * Build model\n  * Assess model by computing error\n  \n* <strong>Week 4 - Evaluation of Alternatives</strong>\n  * Evaluate alternative approaches to clustering\n  * Review process and next steps\n  \n\n<br/>\n### 2.2) Task Breakdown\nIt is anticipated that 10 hours will be allocated each week to perform the tasks required for completion of the project.\n<br/>\n#### 2.2.1) Data Understanding (Week 1)\nThe data understanding task involves the following steps: \n\n\n* Data Review  (1 hour)\n  * Review all tables to develop understanding of data \n\n* Data verification (3 hours)\n  * Meet with Mining Professors to better understand data\n  * Eliminate data not required or redundant \n  * Clarify required analyses \n\n* Exploratory analysis\n  * Completed by J.Ponn for select tables\n\n* Project planning (3 hours)\n  * Setup notebook\n  * Identify breakdown of tasks for each iteration\n  * Allocate time estimates for tasks\n\n* Revisions (1 hour)\n  * Group discussions about project plans\n  * Revisions based on feedback and discussion decisions\n\n* Misc (2 hours)\n  * Buffer for unexpected problems, or changes based on group discussion for this iteration\n  \n#### 2.2.2) Data Preparation (Week 2)\nThe data preparation task involves the following steps: \n\n\n* Data Cleaning (3 hours)\n  * Identify tables that need to be cleaned : the important tables are smry_pen_rate, smry_geotech, smry_movement, smry_bmm\n  * Normalize the text, remove duplicate or error entries\n* Feature extraction and pattern representation (3 hours)\n  * Identify features to be used for input to the model.\n  * Pattern representation consists of defining the number of classes and features to be used in the clustering algorithm.\n\n* Finalize modeling techniques to be used (2 hours)\n  * Model to be considered is the <strong> K-Means</strong>.  \n  * The spark.mllib implementation includes a parallelized variant of the k-means++ method called kmeans||.   \n* Misc. (2 hour)\n  * Group discussion, professor feedback, corrections to notebook \n\n#### 2.2.3) Modeling (Week 3)\nThe data modeling task involves the following steps: \n\n* Integrate and format data for model (2 hours)\n  * Identify the steps needed to input the data into the model. \n  * Modify features to be used to ensure they can be passed to the model.\n* Build model (5 hours)\n  * Run the model, identify clusters\n* Assess model (2 hours)\n  * Perform validity analysis on the clusters produced\n  * Ensure that clusters are tightly bound\n  * Compute Within Set of Squared Error (WSSSE)\n     \n\n#### 2.2.3) Evaluation of Alternatives (Week 4)\nThe evaluation task involves the following steps: \n\n* Evaluate results (6 hours)\n  * Run several iterations on the k-means model, with varying K. Use 'Elbow Method' to identify optimal K.\n  * Perform other iterations of clustering algorithm.\n* Review process and next steps (4 hours)\n  * Make changes to the model as necessary. \n  * Draft conclusions and recommendations for next steps\n* Misc.\n  * Group discussion, professor feedback, corrections to notebook."],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"understanding\"></a>\n## 3.0) Data Understanding"],"metadata":{}},{"cell_type":"markdown","source":["### 3.1) Data Loading"],"metadata":{}},{"cell_type":"markdown","source":["Upload data into databricks, set implicits and define functions to be used to ensure data will be in the type required for processing."],"metadata":{}},{"cell_type":"code","source":["# Import necessary libraries\nimport pandas as pd # For csv I/O and data formatting\nimport re # To search through strings\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nimport java.sql.Timestamp\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\n// val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n//      import sqlContext.implicits._\n\n\n// patching the String class with new functions that have a defualt value if conversion to another type fails.\nimplicit class StringConversion(val s: String) {\ndef toTypeOrElse[T](convert: String=>T, defaultVal: T) = try {\n    convert(s)\n  } catch {\n    case _: Throwable => defaultVal\n  }\n  \n  def toIntOrElse(defaultVal: Int = 0) = toTypeOrElse[Int](_.toInt, defaultVal)\n  def toDoubleOrElse(defaultVal: Double = 0D) = toTypeOrElse[Double](_.toDouble, defaultVal)\n  def toDateOrElse(defaultVal: java.sql.Timestamp = java.sql.Timestamp.valueOf(\"1970-01-01 00:00:00\")) = toTypeOrElse[java.sql.Timestamp](java.sql.Timestamp.valueOf(_), defaultVal)\n}\n\n//Fix the date format in this dataset\ndef fixDateFormat(orig: String): String = {\n    val splited_date = orig.split(\" \")\n    val fixed_date_parts = splited_date(0).split(\"-\").map(part => if (part.size == 1) \"0\" + part else part)\n    val fixed_date = List(fixed_date_parts(0), fixed_date_parts(1), fixed_date_parts(2)).mkString(\"-\")\n    val fixed_time = splited_date(1).split(\":\").map(part => if (part.size == 1) \"0\" + part else part).mkString(\":\")\n    fixed_date + \" \" + fixed_time + \":00\"\n}"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nval bmmRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFQnhJY0pfclBLUmM\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n\nval geotechRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFTjN5WmQ3MmsyTEE\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n\nval load_timeRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFdS0yU29YdkhiWjQ\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n\nval movementRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFMUhEMjJoNk1JZTg\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n\nval truckRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFSkRhc2ZrZDM2aUk\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n\nval pen_rate_summRDD = sc.parallelize( IOUtils.toString( new URL(\"https://drive.google.com/uc?export=download&id=0BzQSUz4yGQIFS3J3WFJPV21ucW8\"), Charset.forName(\"utf8\")).split(\"\\n\"))\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\ncase class geotech(             \n    Level: Int,\n    Blast_Number: String,                                     \n    gd_1: String,\n    gd_2: String,\n    rqd: Double,\n    fracs: Double,\n    is_50: Double,\n    Q: Double\n  )\n\ndef getgeotechCleaned(row:Array[String]):geotech = {\n  return geotech(\n    row(0).toIntOrElse(),\n    row(1),\n    row(2),\n    row(3),\n    row(4).toDoubleOrElse(),\n    row(5).toDoubleOrElse(),\n    row(6).toDoubleOrElse(),\n    row(7).toDoubleOrElse()\n  )\n}\n\nval geotech_data = geotechRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = geotech_data.filter(s => s(0) != \"Level\").map(s => getgeotechCleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_geotech\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql -- View the Geotech Summary Table\nSELECT *\nFROM smry_geotech\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\ncase class load_time(               \n    BLAST_LOC: String,\n    LVL: Int,\n    BLAST_NO: Int,\n    HS_AVG: Double,\n    HS_QT_90: Double,\n    HS_QT_10: Double,\n    HS_MED: Double,\n    HS_NUM_TRUCKS: Double,\n    RS_AVG: Double,\n    RS_QT_90: Double,\n    RS_QT_10: Double,\n    RS_MED: Double,\n    RS_NUM_TRUCKS: Double\n  )\n  \ndef getload_timeCleaned(row:Array[String]):load_time = {\nreturn load_time(\n    row(0),\n    row(1).toIntOrElse(),\n    row(2).toIntOrElse(),\n    row(3).toDoubleOrElse(),\n    row(4).toDoubleOrElse(),\n    row(5).toDoubleOrElse(),\n    row(6).toDoubleOrElse(),\n    row(7).toDoubleOrElse(),\n    row(8).toDoubleOrElse(),\n    row(9).toDoubleOrElse(),\n    row(10).toDoubleOrElse(),\n    row(11).toDoubleOrElse(),\n    row(12).toDoubleOrElse()\n  )\n}\n\n\nval loading_time = load_timeRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = loading_time.filter(s => s(0) != \"BLAST_LOC\").map(s => getload_timeCleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_loading_time\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql -- View the Loading Time Summary Table\nSELECT *\nFROM smry_loading_time\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\ncase class movement(                 \n   BLAST_LOC: String,\n   LOC: String,\n   BENCH_HT: Int,\n   HOLE_DIAM:Int,\n   BURDEN: Int,\n   SPACING: Int,\n   HOLE_CONFIG: String,\n   CONFINEMENT: String,\n   PATTERN: String,\n   STEMMING: Double,\n   POWDER_FAC: Double,\n   IR_TIME: Int,\n   IH_TIME: Int,\n   INITIATION_TYPE: String,\n   EXPLOSIVE_TYPE: String,\n   ROCK_TYPE: String,\n   BMM: Int,\n   COLLAR_NORTH: Double,\n   COLLAR_EAST: Double,\n   COLLAR_RL: Double,\n   PRE_BMM_RL: Double,\n   INST_DEPTH: Double,\n   AFTER_EAST: Double,\n   AFTER_NORTH: Double,\n   SURFACE_RL: Double,\n   POST_BMM_RL: Double,\n   THREED_DIST: Double,\n   H_DIST: Double,\n   V_DIST: Double,\n   DIRECTION: Double,\n   INCLINATION: Double,\n   HEAVE: Double\n  )\n  \ndef getmovementCleaned(row:Array[String]):movement = {\nreturn movement(\n    row(0),\n    row(1),\n    row(2).toIntOrElse(),\n    row(3).toIntOrElse(),\n    row(4).toIntOrElse(),\n    row(5).toIntOrElse(),\n    row(6),\n    row(7),\n    row(8),\n    row(9).toDoubleOrElse(),\n    row(10).toDoubleOrElse(),\n    row(11).toIntOrElse(),\n    row(12).toIntOrElse(),\n    row(13),\n    row(14),\n    row(15),\n    row(16).toIntOrElse(),\n    row(17).toDoubleOrElse(),\n    row(18).toDoubleOrElse(),\n    row(19).toDoubleOrElse(),\n    row(20).toDoubleOrElse(),\n    row(21).toDoubleOrElse(),\n    row(22).toDoubleOrElse(),\n    row(23).toDoubleOrElse(),\n    row(24).toDoubleOrElse(),\n    row(25).toDoubleOrElse(),\n    row(26).toDoubleOrElse(),\n    row(27).toDoubleOrElse(),\n    row(28).toDoubleOrElse(),\n    row(29).toDoubleOrElse(),\n    row(30).toDoubleOrElse(),\n    row(31).toDoubleOrElse()\n  )\n}\n\nval smry_movement = movementRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = smry_movement.filter(s => s(0) != \"BLAST_LOC\").map(s => getmovementCleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_movement\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql -- View the Summary Movement Table\nSELECT *\nFROM smry_movement\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\ncase class bmm(                 \n   BLAST_LOC: String,\n   GEO_DOM1: String,\n   GEO_DOM2: String,\n   RQD: Double,\n   FRACS_FREQ: Double,\n   IS_50: Double,\n   Q: Double,\n   LEVEL: String,\n   PEN_RATE: Double,\n   BLAST_DATE: String,\n   ROCK_TYPE: String,\n   MATERIAL: String,\n   HG_MG: Int,\n   LG: Int,\n   NAG: Int,\n   TOTAL_TONS: Int,\n   BLAST_TYPE: String ,  \n   DH_DELAY: Int,\n   HS_DELAY: Int,\n   RS_DELAY: Int,\n   HOLE_DIA:Int, \n   BENCH_HEIGHT: Int,\n   BURDEN: Int,\n   SPACING: Int,\n   STEMMING: Double,\n   SUB_DRILL: Double ,\n   EXPLOSIVE: String,\n   BLAST_PATTERN: String,\n   TOTAL_EXPLOSIVE: Double,\n   HOLE_DEPTH: Double,\n   FIRING_DIRECTION: String,\n   POWDER_FACTOR: Double,\n   INITIATION_TYPE: String,\n   CONFINEMENT: String,\n   BMM_NUM: Int,\n   THREED_DIST: Double,\n   H_DIST: Double,\n   V_DIST: Double,\n   DIRECTION: Double,\n   INCLINATION: Double,\n   HEAVE: Double \n  ) \n   \n  \ndef getbmmcleaned(row:Array[String]):bmm = {\nreturn new bmm(\n    row(0),\n    row(1),\n    row(2),\n    row(3).toDoubleOrElse(),\n    row(4).toDoubleOrElse(),\n    row(5).toDoubleOrElse(),\n    row(6).toDoubleOrElse(),\n    row(7),\n    row(8).toDoubleOrElse(),\n    row(9),\n    row(10),\n    row(11),\n    row(12).toIntOrElse(),\n    row(13).toIntOrElse(),\n    row(14).toIntOrElse(),\n    row(15).toIntOrElse(),\n    row(16) ,\n    row(17).toIntOrElse(),\n    row(18).toIntOrElse(),\n    row(19).toIntOrElse(),\n    row(20).toIntOrElse(),\n    row(21).toIntOrElse(),\n    row(22).toIntOrElse(),\n    row(23).toIntOrElse(),\n    row(24).toDoubleOrElse(),\n    row(25).toDoubleOrElse(),\n    row(26),\n    row(27),\n    row(28).toDoubleOrElse(),\n    row(29).toDoubleOrElse(),\n    row(30),\n    row(31).toDoubleOrElse(),\n    row(32),\n    row(33),\n    row(34).toIntOrElse(),\n    row(35).toDoubleOrElse(),\n    row(36).toDoubleOrElse(),\n    row(37).toDoubleOrElse(),\n    row(38).toDoubleOrElse(),\n    row(39).toDoubleOrElse(),\n    row(40).toDoubleOrElse() \n  )\n}\n\nval smry_bmm = bmmRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = smry_bmm.filter(s => s(0) != \"BLAST_LOC\").map(s => getbmmcleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_bmm\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql -- View the Summary BMM Table\nSELECT *\nFROM smry_bmm\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\ncase class truck(               \n    BLAST_LOC: String,\n    LVL: Int,\n    BLAST_NO: Int,\n    HS_AVG: Double,\n    HS_QT_90: Double,\n    HS_QT_10: Double,\n    HS_MED: Double,\n    HS_NUM_TRUCKS: Int,\n    RS_AVG: Double,\n    RS_QT_90: Double,\n    RS_QT_10: Double,\n    RS_MED: Double,\n    RS_NUM_TRUCKS: Int\n  )\n  \ndef gettruckCleaned(row:Array[String]):truck = {\nreturn truck(\n    row(0),\n    row(1).toIntOrElse(),\n    row(2).toIntOrElse(),\n    row(3).toDoubleOrElse(),\n    row(4).toDoubleOrElse(),\n    row(5).toDoubleOrElse(),\n    row(6).toDoubleOrElse(),\n    row(7).toIntOrElse(),\n    row(8).toDoubleOrElse(),\n    row(9).toDoubleOrElse(),\n    row(10).toDoubleOrElse(),\n    row(11).toDoubleOrElse(),\n    row(12).toIntOrElse()\n  )\n}\n\nval smry_truck_quantity = truckRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = smry_truck_quantity.filter(s => s(0) != \"BLAST_LOC\").map(s => gettruckCleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_truck_quantity\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql -- View the Summary Truck Quantity Table\nSELECT *\nFROM smry_truck_quantity\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\ncase class penetration_rate_summ(                    \n    BLAST_LOC: String,\n    LVL: String,\n    BLAST_NO: String,\n    X: String,                            \n    Y: String,                                  \n    Z: String,                                   \n    PEN_RATE: Double,\n    ROCK_TYPE: String\n  )\n  \ndef getpenetration_rate_summCleaned(row:Array[String]):penetration_rate_summ = {\nreturn penetration_rate_summ(\n    row(0),\n    row(1),\n    row(2),\n    row(3),\n    row(4),\n    row(5),\n    row(6).toDoubleOrElse(),\n    row(7)\n  )\n}\n\nval pen_rates_summ = pen_rate_summRDD.map(line => line.split(\",\").map(elem => elem.trim))\nval data = pen_rates_summ.filter(s => s(0) != \"BLAST_LOC\").map(s => getpenetration_rate_summCleaned(s)).toDF()\ndata.createOrReplaceTempView(\"smry_pen_rate\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%sql -- View the Summary Penetration Rate Table\nSELECT *\nFROM smry_pen_rate\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### 3.2) Data Exploration"],"metadata":{}},{"cell_type":"markdown","source":["After loading of the data, conduct data exploration to see what is in the tables."],"metadata":{}},{"cell_type":"markdown","source":["### Review of Data within the SMRY_GEOTECH Table"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- How many unique blast levels are there?\nSELECT COUNT(LEVEL)\nFROM smry_geotech"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\n-- Counts of distinct levels in smry_geotech\nSELECT DISTINCT (LEVEL), COUNT(LEVEL) as counts\nFROM smry_geotech\nGROUP BY LEVEL \nORDER BY counts"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The levels range from 148 to 208, and include a new row for each new blasting record. So for example, Level 244 has over 120 blast rows associated with it. See the summary table below."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Distinct number of blast numbers in the smry_geotech table\nSELECT DISTINCT blast_number \nFROM smry_geotech\nGROUP BY blast_number\nORDER BY blast_number DESC"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\n-- Distribution of RQD values\nSELECT rqd\nFROM smry_geotech"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["The largest bin of RQD values is between 90-95 (there are >300) and between 95-100 (almost 450). The values around 0 appear to be errors or blanks and should be removed."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- How many distinct rqd values does the smry_geotech have? \nSELECT distinct(rqd)\nFROM smry_geotech\nGROUP BY rqd"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Since there are only 93 distinct rqd values, it appears that these values have been summarized and applied to many more blast locations."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Distribution of fracs\nSELECT fracs\nFROM smry_geotech"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["The number of fracs appear to be distributed around 5, with a range from 0 - 14."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Distribution of is_50 values\nSELECT is_50\nFROM smry_geotech"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Distribution of is50 values is around 9, with a range from 4 - 16. The values around 0 appear to be erroneous and should be removed."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Distribution of q values\nSELECT q\nFROM smry_geotech"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Distribution of q values is around 14, with the range mostly between 5 - 25. The values around 0 appear to be erroneous and should be removed."],"metadata":{}},{"cell_type":"markdown","source":["### Review of Data within the smry_pen_rate table"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- How many individual blast locaitons are there?\nSELECT distinct (BLAST_LOC)\nFROM smry_pen_rate"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["The levels in smry_pen_rate range from 148 - 268, with four records for levels that have alpha-numeric characters. These should be removed."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Counts of distinct levels \nSELECT DISTINCT (lvl), COUNT(lvl) as counts\nFROM smry_pen_rate\nGROUP BY lvl \nORDER BY counts"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["Number of levels range from 160-234 which is similar to that shown in the smry_geotech. There are some discrepancies, such as the alphabetic characters. These should be removed."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Counts of distinct levels \nSELECT DISTINCT (BLAST_NO), COUNT(BLAST_NO) as counts\nFROM smry_pen_rate\nGROUP BY BLAST_NO \nORDER BY counts"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["This table should correspond to the blast_number summary from smry_geotech above. However it's clear that there are large differences. For example, in this table there are many values with alphabetic characters. Ideally they should have a three digit numeric character. These will need to be normalized or removed during cleaning."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Distribution of pen_rate values\nSELECT pen_rate\nFROM smry_pen_rate"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["There are many penetrations rates above 2. It is unclear if these are erroneous and should be exploded. Check with the mining professor."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Rock type classifications\nSELECT DISTINCT rock_type, count(rock_type)\nFROM smry_pen_rate\nGROUP BY rock_type"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["The majority of rock type classification is mf, followed by kp and ovb."],"metadata":{}},{"cell_type":"markdown","source":["### Review of Data within the smry_bmm table"],"metadata":{}},{"cell_type":"markdown","source":["Following discussions with Prof. Esmaeili and B.Ohadi, another data table was provided (smry_bmm). This table contains geotechnical data, blast parameters and blast outcomes for distinct blast locations. The data was compiled by B.Ohadi."],"metadata":{}},{"cell_type":"code","source":["%sql -- Let's see what's in the smry_bmm table\nSELECT *\nFROM smry_bmm\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\n-- See how many blast locations per blast level\nSELECT LEVEL, COUNT(LEVEL)\nFROM smry_bmm\nGROUP BY LEVEL\nSORT BY COUNT(LEVEL) DESC"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["Some levels have a large number of blasts - for example, 208 and 256, while some levels have far fewer blast data associated."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- See distribution of RQD\nSELECT RQD\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Based on visual assessment of the RQD, it appears that the data has been taken from the smry_geotech table. As that table has already been explored above, the subsequent visualizations will focus on the non-geotech parameters."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Box plot of Total Tons distribution\nSELECT TOTAL_TONS\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Median tonnage is around 320,00o T, with 75% of data occuring within 200,000 to 600,000 T."],"metadata":{}},{"cell_type":"code","source":["%sql --See distribution of IS_50 values\nSELECT is_50\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%sql\n-- What Blast Types are there?\nSELECT DISTINCT Blast_Type, COUNT(*)\nFROM smry_bmm\nGROUP BY BLAST_TYPE"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Majority of blast types are for production"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most prominant HS_delay used in the blasting process?\nSELECT DISTINCT HS_DELAY, COUNT(*)\nFROM smry_bmm\nGROUP BY HS_DELAY"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["A blasting delay of 42 ms was most used."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most prominant RS_delay used in the blasting process?\nSELECT DISTINCT RS_DELAY, COUNT(*)\nFROM smry_bmm\nGROUP BY RS_DELAY"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["The most common blasting dleay was 100 ms."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Lets visualize the hole diameter distributions\nSELECT DISTINCT HOLE_DIA, COUNT(*)\nFROM smry_bmm\nGROUP BY HOLE_DIA"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["The greatest proportion of hole diameter was 216 mm."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common burden? \nSELECT DISTINCT BURDEN, COUNT(*)\nFROM smry_bmm\nGROUP BY BURDEN"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["Burden of 6 m was used in almost all cases. Values of 0 are likely erroneous and should be removed."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common bencht height? \nSELECT DISTINCT BENCH_HEIGHT, COUNT(*)\nFROM smry_bmm\nGROUP BY BENCH_HEIGHT"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["Almost all bench heights were 12 m."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common spacing? \nSELECT DISTINCT SPACING, COUNT(*)\nFROM smry_bmm\nGROUP BY SPACING"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Most common stemming height was 7 m."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common stemming? \nSELECT DISTINCT STEMMING, COUNT(*)\nFROM smry_bmm\nGROUP BY STEMMING"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["Most common stemming was 4.5 m, followed by 5 m and 4 m."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common explosive? \nSELECT DISTINCT EXPLOSIVE, COUNT(*)\nFROM smry_bmm\nGROUP BY EXPLOSIVE"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["Almost all explosive used was th Fortis Extra 1.05"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What types of blast patterns were used? \nSELECT DISTINCT BLAST_PATTERN, COUNT(*)\nFROM smry_bmm\nGROUP BY BLAST_PATTERN"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["Almost all th blast patterns were staggered."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is the most common hole depth? \nSELECT DISTINCT HOLE_DEPTH, COUNT(*)\nFROM smry_bmm\nGROUP BY HOLE_DEPTH"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["Hole depth ranged from 1 to 8 m, with most holes drilled to 7 m depth."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is breakdown of blast initiation types? \nSELECT DISTINCT INITIATION_TYPE, COUNT(*)\nFROM smry_bmm\nGROUP BY INITIATION_TYPE"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Most initiation types were \"V\" followed by \"Echelon\""],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is breakdown of confinement? \nSELECT DISTINCT CONFINEMENT, COUNT(*)\nFROM smry_bmm\nGROUP BY CONFINEMENT"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["Two types of confinement are presented. Free face and Choked. The values need to be normalized."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is distribution of three d distances? \nSELECT THREED_DIST\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["Most 3d-distance values range between 3.6 to 5.1 m."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is distribution of horizontal distances? \nSELECT H_DIST\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["Similarly, most horizontal distances range from 3.1 m to 4.1 m"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is distribution of vertical distances? \nSELECT V_DIST\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["Vertical distance has a greater fluctuation, and a median around 1.5 m."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- What is distribution of heave? \nSELECT heave\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["Most heave was measured between 2 and 5 m."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT COUNT(BLAST_LOC)\nFROM smry_bmm"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["### Data Selection\nBased on a review of the tables, the smry_bmm table was selected for further analysis. This table was selected as it contains already contains the necessary elements to be assessed with the clustering technique. As it was shown, the Table contains:\n* Geotechnical data (Such as RQD, FRAQS_FREQ, Q, and PEN_RATE)\n* Blast data (Such as BLAST_LOC, LEVEL)\n* Rock data (Such as ROCK_TYPE, MATERIAL)\n* Blast parameters (Such as BENCH_HEIGHT, SPACING, STEMMING, SUB_DRILL, POWDER_FAC, INITIATION)\n* Blast outcomes (Such as D_Dist, V_Dist, HEAVE )"],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"preparation\"></a>\n## 4.0) Data Preparation"],"metadata":{}},{"cell_type":"markdown","source":["### 4.1) Data Cleaning"],"metadata":{}},{"cell_type":"markdown","source":["####List of Cleaning to be Performed \n* Remove GEO_DOM, GEO_DOM1 - These columns appear to not provide any further useful information \n* Remove blast locations where no RQD, FRAQS_FREQ, IS_50 or Q exist, or are 0 - Geotech parameters are an important component of the clustering process\n* Remove LEVEL column - not required for analysis \n* Remove BLAST_DATE column - not required for analysis \n* Normalize BLAST PATTERN & CONFINEMENT labels"],"metadata":{}},{"cell_type":"code","source":["#Convert spark df to pandas, do not select GEO_DOM, GEO_DOM1, BLAST_DATE, LEVEL, BMM_NUM \npDf = sqlContext.sql(\"SELECT BLAST_LOC, RQD, FRACS_FREQ, IS_50, Q, PEN_RATE, ROCK_TYPE, BLAST_TYPE, DH_DELAY, HS_DELAY, RS_DELAY, HOLE_DIA, BENCH_HEIGHT, BURDEN, SPACING, STEMMING, SUB_DRILL, EXPLOSIVE, BLAST_PATTERN, TOTAL_EXPLOSIVE, HOLE_DEPTH, POWDER_FACTOR, INITIATION_TYPE, CONFINEMENT, THREED_DIST, H_DIST,V_DIST, DIRECTION, INCLINATION, HEAVE FROM smry_bmm \").toPandas()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["#Confirm that appropriate columns selected\npDf.head()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["#Lets whats in the dataframe\npDf.info()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["#Remove the rows where RQD, FRACS_FREQ, IS_50, Q and PEN_RATE are zeros\npDf = pDf[(pDf.RQD > 0) & (pDf.FRACS_FREQ > 0) & (pDf.IS_50 > 0) & (pDf.Q > 0) & (pDf.PEN_RATE > 0)]\n#We only have 258 rows in the dataframe\npDf.shape"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["# Normalize string labels by removing excessive whitespace in the middle of a word, shifting to lowercase, and removing any whitespace after the text\ndef normalizeLabels(label):\n  return re.sub('[ ]+',' ',label).lower().rstrip().lstrip()"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["#Make the labels lowercase and strip blank spaces\npDf['CONFINEMENT'] = pDf['CONFINEMENT'].apply(normalizeLabels)\npDf['BLAST_TYPE'] = pDf['BLAST_TYPE'].apply(normalizeLabels)\npDf['ROCK_TYPE'] = pDf['ROCK_TYPE'].apply(normalizeLabels)\npDf['EXPLOSIVE'] = pDf['EXPLOSIVE'].apply(normalizeLabels)\npDf['BLAST_PATTERN'] = pDf['BLAST_PATTERN'].apply(normalizeLabels)\npDf['INITIATION_TYPE'] = pDf['INITIATION_TYPE'].apply(normalizeLabels)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["#Confirm that the labels are normalized\npDf.head()"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["<a id=\"modeling\"></a>\n## 5.0) Modeling"],"metadata":{}},{"cell_type":"markdown","source":["### 5.1) Feature Extraction"],"metadata":{}},{"cell_type":"markdown","source":["According to Hudaverdi [4] the ratios to be used for clustering are :\n* Spacing (S) / Burden (B)\n* Bench Height (H) / Burden (B)\n* Burden (B) / Hole Diameter (D)\n* Stemming (T) / Burden (B)\n* Subdrill (U) / Burden (B)\n* Powder Factor"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT *\nFROM smry_bmm\nLIMIT 5"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["#Calculate ratios\npDf['SB_ratio'] = pDf['SPACING'] / pDf['BURDEN']\npDf['HB_ratio'] = pDf['BENCH_HEIGHT'] / pDf['BURDEN']\npDf['BD_ratio'] = pDf['BURDEN'] / pDf['HOLE_DIA']\npDf['TB_ratio'] = pDf['STEMMING'] / pDf['BURDEN']\npDf['UB_ratio'] = pDf['SUB_DRILL'] / pDf['BURDEN']"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["#Inspect dataframe to ensure the ratios have been calculated\npd.set_option('display.max_columns', None)\npDf.head()"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["# Convert a label feature into an index number\ndef facorizeFeature(feat, df, labelDict):\n  # Convert string labels into an index number\n  labels, levels = pd.factorize(df[feat])\n  \n  # Add index numbers to main dataframe\n  df[feat] = labels\n  \n  # Store labels in dictionary for later reference\n  labelDict[feat] = levels\n  \n  return df, labelDict"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["# Initalize label dictionary\nmLabelDict = {}"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["# Apply label normalization to data\npDf, mLabelDict = facorizeFeature('ROCK_TYPE', pDf, mLabelDict)\npDf, mLabelDict = facorizeFeature('BLAST_TYPE', pDf, mLabelDict)\npDf, mLabelDict = facorizeFeature('EXPLOSIVE', pDf, mLabelDict)\npDf, mLabelDict = facorizeFeature('BLAST_PATTERN', pDf, mLabelDict)\npDf, mLabelDict = facorizeFeature('INITIATION_TYPE', pDf, mLabelDict)\npDf, mLabelDict = facorizeFeature('CONFINEMENT', pDf, mLabelDict)"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["#Check the dataframe to ensure strings have been factorized\npDf.head()"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["#See the indexed labels\nmLabelDict"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["#Drop columns that are not required for the clustering process\nfeatures = pDf.drop(['BLAST_LOC','DIRECTION','INCLINATION','HEAVE'],1)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["#Drop columns that include blast movement as these are outcomes \nfeatures.drop(['THREED_DIST','H_DIST', 'V_DIST'],1,inplace='true')"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["#Drop columns that created ratios for\nfeatures.drop(['HOLE_DIA','BENCH_HEIGHT','BURDEN','SPACING','STEMMING','SUB_DRILL' ],1,inplace='true')"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["#View the dataframe to ensure the correct columns are in place\nfeatures.tail()"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["#Count the number of times that infinity occurs in the dataset\nnp.isinf(features).sum()"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["#Check if the dataframe has any null values \nfeatures.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["#Convert infinities to NaN's so they can all be handled together\nfeatures.replace(np.inf, np.nan,inplace='True')"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["#Confirm infinities are removed\nnp.isinf(features).sum()"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["#Remove the NaN rows from the dataframe\nfeatures.dropna(inplace='True')"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["#Check the shape of the features dataframe now that the NaN's have been dropped\nfeatures.shape"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["#Create a list of the features \nfeaturesList = list(features)"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["#Let's see the features list to ensure it has all the features we want\nfeaturesList"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"markdown","source":["### 5.2) Train the Model - Base Case"],"metadata":{}},{"cell_type":"markdown","source":["The first iteration of the model involves trialling with all the initial set of data and with a randomly selected k. The results from this assessment are not representative of the true clustering.\n\nThe parameters to be used are those as default in the spark.mllib implementation guide. Max iterations have been increased to 100, and the initialization mode is kept as random."],"metadata":{}},{"cell_type":"code","source":["#Import necessary libraries \nfrom pyspark.mllib.clustering import KMeans, KMeansModel\nfrom numpy import array\nfrom math import sqrt\nfrom pyspark.mllib.stat import Statistics\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport numpy as np\nfrom sklearn import cluster\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.feature import StandardScaler"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["#Parse the features pandas dataframe into an RDD of features which will be input into the model\nfeaturesRDD = sqlContext.createDataFrame(features).rdd.map(lambda row: np.array([float(item) for item in row]))"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["#Let's see some summary statistics of the Features RDD\nsummary = Statistics.colStats(featuresRDD)\nprint(summary.mean())  # a dense vector containing the mean value for each column\nprint(summary.variance())  # column-wise variance\nprint(summary.numNonzeros())  # number of nonzeros in each column"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"markdown","source":["As it can be seen - several columns have a variance of 0, indicating that these columns will likely not add substantially to the clustering. The columns are columns 8 - 12. In later iterations, these columns will be removed."],"metadata":{}},{"cell_type":"code","source":["#Train the clustering algorithm using the Features RDD. Try with 4 clusters in the first iteration.\nclusters = KMeans.train(featuresRDD, 4, maxIterations=100, initializationMode=\"random\")"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusters.centers[clusters.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["#Computer the error \nWSSSE = featuresRDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["#Let's look at the coordinates of the centre points of the clusters, to help understand which features are the most discriminatory. \nd = clusters.clusterCenters\nclusterDf = pd.DataFrame(d)\n#Pass the feautures list as column labels \nclusterDf.columns = featuresList"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["#View the cluster centre coordinates for all four clusters\nclusterDf"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["#View a stastical summary of the cluster centre coordinates\nclusterDf.describe()"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"markdown","source":["Without normalization of the data, it's not very useful to describe mean and distribution of the data. However, based on an visual inspection of the features, the following features appear to be the most important in determining clusters, based on the differences between their cluster coordinates:\n* FRACS_FREQ\n* IS_50\n* Q\n* PEN_RATE\n\nMost of the blast parameters have very similar cluster centre coordinates. This indicates that the blast parameters are not very useful in helping to determine cluster assignments."],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"evaluation\"></a>\n## 6.0) Alternative Iterations of Clustering"],"metadata":{}},{"cell_type":"markdown","source":["### 6.1) Increasing K Sequentially"],"metadata":{}},{"cell_type":"markdown","source":["In order to evaluate the best number of clusters, the Elbow Method approach is carried out. Starting from k=1, the K-means algorithm is run on the same dataset with increasing k and observing the value of the cost function WSSE. The number of clusters selected should be 1 greater than the 'elbow' on the graph, indicating the biggest drop of WSSE."],"metadata":{}},{"cell_type":"code","source":["#Initialize the empty array\ncluster_elbow = []\nerrorGraph = []"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["#Define the function which will be used to compute the cost for each iteration\ndef error_iter(clusters_iter, point):\n    center = clusters_iter.centers[clusters_iter.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["#define function to compute WSSE as K increases sequentially \ndef num_k_iter(num_k, fRDD):\n    for k in range(1,num_k):\n      #Train the model, with k increasing from 1 to entered value\n      clusters_iter = KMeans.train(fRDD, k, maxIterations=100, initializationMode=\"random\")\n      #Calculate the WSSE errors for each k\n      WSSE_iter = fRDD.map(lambda point: error_iter(clusters_iter, point)).reduce(lambda x, y: x + y) \n      #Append it to an initialized list   \n      errorGraph.append([k,WSSE_iter])   "],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["#Let's try out the funcion with k from 1 to 10\nnum_k_iter(10,featuresRDD)"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["#Let's see what's in the Cluster Elbow array\ncluster_elbow = errorGraph"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["#Create a dataframe so we can viusalize the Cluster Elbow graph\ncluster_df = sqlContext.createDataFrame(cluster_elbow)\ncluster_df.createOrReplaceTempView('elbows')"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["%sql -- Plot the number of K vs. the WSSE\nSELECT _1 as NumK, _2 as WSSSE\nFROM elbows"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"markdown","source":["The biggest drop occurs after the 2'nd cluster so this analysis suggests that 3 clusters would be the optimal solution for this dataset."],"metadata":{}},{"cell_type":"markdown","source":["### 6.2) With Normalized Columns in Features Dataframe"],"metadata":{}},{"cell_type":"markdown","source":["Previously it was noted that features have large differences in actual values. For example RQD values are mostly in the range from 80 - 100, while FRACS_FREQ are largely between 2 - 6.These differences may cause discrepancies in the clustering process and should therefore be normalized. \n\nThe method used by Hudaverdi [1] to normalize data is the Z-score normalization technique. In this method, each value in the feature table is adjusted by subtracting the column mean and dividing by the column standard deviation."],"metadata":{}},{"cell_type":"code","source":["#Create a function that will normalize each value in the dataframe. Normalization involves subtracting each value by the column mean and dividing by the column standard deviation. \ndef normalize(df):\n    features_normalized = df.copy()\n    for feature_name in df.columns:\n        col_mean = df[feature_name].mean()\n        col_std = df[feature_name].std()\n        features_normalized[feature_name] = (df[feature_name] - col_mean) / col_std\n    return features_normalized"],"metadata":{},"outputs":[],"execution_count":156},{"cell_type":"code","source":["#Let's see the original features table prior to normalization\nfeatures.head()"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"code","source":["#Pass the features to be normalized to the function\nnormalized_features = normalize(features)\n#See the new normalized values \nnormalized_features.head()"],"metadata":{},"outputs":[],"execution_count":158},{"cell_type":"markdown","source":["After normalization, a few columns appear to produce a lot of NaN values. This occus because all the values in these colums are the same, therefore subtracting the mean results in 0. Let's see how many are NaNs."],"metadata":{}},{"cell_type":"code","source":["#View the null values in the normalized_features dataframe\nnormalized_features.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":160},{"cell_type":"code","source":["#Remove NaN columns from normalized_feautres\nnormalized_features.drop(['DH_DELAY','HS_DELAY','RS_DELAY','EXPLOSIVE','BLAST_PATTERN'],1,inplace='true')"],"metadata":{},"outputs":[],"execution_count":161},{"cell_type":"code","source":["#Create a list of the column headers \nnormalized_feautresList = list(normalized_features)\n#Let's view the dataframe\nnormalized_features.head()"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"code","source":["#Parse the features pandas dataframe into an RDD of features which will be input into the model\nnormalized_featuresRDD = sqlContext.createDataFrame(normalized_features).rdd.map(lambda row: np.array([float(item) for item in row]))"],"metadata":{},"outputs":[],"execution_count":163},{"cell_type":"code","source":["#Run the clustering function \nnum_k_iter(10,normalized_featuresRDD)"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["#Pass the error graph to a new variable\nnormalizedElbow = errorGraph"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["#Create a spark dataframe so we can viusalize the Cluster Elbow graph\nNormCluster_df = sqlContext.createDataFrame(normalizedElbow)\nNormCluster_df.createOrReplaceTempView('norm_elbows')"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["%sql -- Plot normalized WSSE Error vs. Num K\nSELECT _1 as NumK, _2 as WSSSE\nFROM norm_elbows"],"metadata":{},"outputs":[],"execution_count":167},{"cell_type":"markdown","source":["Based on this graph, the node after largest drop is 3, therefore it's recommended that 3 clusters be used for this dataset. Let's conduct some more exploration with 3 clusters and see how the data look."],"metadata":{}},{"cell_type":"code","source":["#Re run the model training with 3 clusters specified\nnormClusters = KMeans.train(normalized_featuresRDD, 3, maxIterations=100, initializationMode=\"random\")"],"metadata":{},"outputs":[],"execution_count":169},{"cell_type":"code","source":["#Create an array of cluster centers\nnormCentres = normClusters.clusterCenters\n#Pass the array to a pandas dataframe\nnormCentresDf = pd.DataFrame(normCentres)"],"metadata":{},"outputs":[],"execution_count":170},{"cell_type":"code","source":["#Add column headers to allow for visual assessment of features\nnormCentresDf.columns = normalized_feautresList\n#Let's view the centre coordinates of each normalized table\nnormCentresDf"],"metadata":{},"outputs":[],"execution_count":171},{"cell_type":"markdown","source":["The centre coordinates indicate the location of each cluster. Although we don't know the tightness of the cluster, we can use the centre coordinates to visualize the splits among the features."],"metadata":{}},{"cell_type":"code","source":["#Create a spark dataframe so we can viusalize the Cluster Elbow graph\nNormClusterCentre_df = sqlContext.createDataFrame(normCentresDf)\nNormClusterCentre_df.createOrReplaceTempView('norm_cluster_cent')"],"metadata":{},"outputs":[],"execution_count":173},{"cell_type":"code","source":["%sql -- Let's see all the columns in our centre cluster dataframe\nSELECT *\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":174},{"cell_type":"code","source":["%sql -- Differences in RQD values among the normalized clusters shows three relatively distinct groupings of clusters \nSELECT RQD\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["%sql -- Differences in FRACS_FREQ values among the normalized clusters shows three distinct clusters.\nSELECT FRACS_FREQ\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["%sql -- Differences in IS_50 values among the normalized clusters shows that clusters 0 and 1 are more similar to each other than to 2\nSELECT IS_50\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":177},{"cell_type":"code","source":["%sql -- Differences in Q values among the normalized clusters shows that 0,2 are more similar to each other than to 1. \nSELECT Q\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":178},{"cell_type":"code","source":["%sql -- Differences in PEN_RATE values among the normalized clusters shows that clusters 0 and 2 are more similar to each other than to 0\nSELECT PEN_RATE\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":179},{"cell_type":"code","source":["%sql -- Differences in RQD values among the normalized clusters shows that clusters 1 and 2 are more similar to each other than to 0\nSELECT ROCK_TYPE\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":180},{"cell_type":"code","source":["%sql -- Differences in confinement values among the normalized clusters shows that clusters 0 and 2 are more similar to each other than to 2\nSELECT CONFINEMENT\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"code","source":["%sql -- Differences in ROCK_TYPE values among the normalized clusters shows that clusters 1 and 2 are more similar to each other than to 0\nSELECT ROCK_TYPE\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":182},{"cell_type":"code","source":["%sql -- Differences in SB_Ratio values among the normalized clusters shows that only 1 cluster was formed.\nSELECT SB_RATIO\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["%sql -- Differences in HB_RATIO values among the normalized clusters shows that only 1 cluster was formed.\nSELECT HB_RATIO\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"code","source":["%sql -- Differences in TB_RATIO values among the normalized clusters shows that only 1 cluster was formed. \nSELECT TB_RATIO\nFROM norm_cluster_cent"],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"markdown","source":["### 6.3) With Just the Geotechnical Parameters"],"metadata":{}},{"cell_type":"markdown","source":["Based on the results of the previous clustering, it appears that many of the blast parameters are not as useful for assigning discriminant clusters. As such, we should try assign clusters with just the geotechnical parameters to see if there are interesting groupings within the data."],"metadata":{}},{"cell_type":"code","source":["#Create a geotech data features list\nnormalizedGeotech = normalized_features[['RQD','FRACS_FREQ','Q','PEN_RATE','ROCK_TYPE']]"],"metadata":{},"outputs":[],"execution_count":188},{"cell_type":"code","source":["#Let's view the new table\nnormalizedGeotech.head()"],"metadata":{},"outputs":[],"execution_count":189},{"cell_type":"code","source":["#Ensure it has the correct shape\nnormalizedGeotech.shape"],"metadata":{},"outputs":[],"execution_count":190},{"cell_type":"code","source":["#Create the RDD\nnormalized_geofeaturesRDD = sqlContext.createDataFrame(normalizedGeotech).rdd.map(lambda row: np.array([float(item) for item in row]))"],"metadata":{},"outputs":[],"execution_count":191},{"cell_type":"code","source":["#Pass it to the clustering function\nnum_k_iter(10,normalized_geofeaturesRDD)"],"metadata":{},"outputs":[],"execution_count":192},{"cell_type":"code","source":["#Create an elbow graph with the WSSSE error points\nnormalized_geoElbow = errorGraph"],"metadata":{},"outputs":[],"execution_count":193},{"cell_type":"code","source":["#Create a spark dataframe so error points can be visualized\nNormgeoCluster_df = sqlContext.createDataFrame(normalized_geoElbow)\nNormgeoCluster_df.createOrReplaceTempView('norm_geo_elbows')"],"metadata":{},"outputs":[],"execution_count":194},{"cell_type":"code","source":["%sql -- Let's view the WSSE vs. Num K\nSELECT _1 as NumK, _2 as WSSSE\nFROM norm_geo_elbows"],"metadata":{},"outputs":[],"execution_count":195},{"cell_type":"markdown","source":["Similar to previous iterations, it appears the best K is 3 indicating that the data can be grouped into three distinct clusters based on the geotechnical features."],"metadata":{}},{"cell_type":"markdown","source":["## 7.0) Conclusions & Recommendations\n\nBased on analysis conducted, the following observations are made :<br/>\n* The K-means algorithm is useful for grouping data based on underlying similarity \n* Based on running the K-means algorithm with sequentially increasing K, the optimal number of clusters was found to be 3. This was also found with just the geotechnical dataset - which indicates validation of the initial test. This suggests that there are 3 underlying groupings of the data.\n* Upon reviewing the centroids of the coordinates, it was typically noted that two of the clusters were closer to each other than the third, however this alternated depending on which parameter was assessed. \n* The blast parameters did not provide useful discriminant data to perform the clustering. This was shown by utilizing just the geotechnical data and obtaining 3 clusters - the same result as conducted with blast parameters. This suggests that most blasts have very similar data points, and is supported by the graphical plots depicted in Section 3.\n\n\nThe following recommendations are made regarding data management and future work:\n* The dataset used for the final clustering was relatively small - only 217 points. This was as a result of data processing and cleaning which resulted in the removal of many datapoints. In order to build better models, a larger more accurate dataset should be collected. \n* The dataset used for the analysi (smry_bmm) contained many data that had been averaged from one location across many different points. This is understandable considering the limitations of data collections at a mine, however additional efforts should be made to collect discrete geospatial data which are representative - for example the RQD, Q, IS_50 attached to individual blast hole data points. \n* This analysis provided distinct clusters into which the data can be grouped based on similarity. Further work can develop regression models to predict movement or fragmentation based on the clusters. These will likely result in better predictive models. \n* Documentation for ML Lib is still being developed (or difficult to find) as such, detailed analysis on clusters was not possible. For example it would have been useful to see cluster assignments and relate it back to individual data points."],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"bibliography\"></a>\n## 7.0) Bibliography\n<strong>Selected Paper:</strong><br/>\n<a id=\"ref1\"></a>\n[1] T. Hudaverdi. Application of Multivariate Analysis for Prediction of Blast Induced Ground Vibrations. In Soil Dynamics and Earthquake Engineering. Pages 300  308, 2012.<br/><br/>\n<a id=\"ref2\"></a>\n\n<strong>Also cited in literature review:</strong><br/>\n[2] M. Rezaei, M. Monjezi, A. Varjani. Development of a fuzzy model to predict flyrock in surface mining. In Safety Science, pages 298-305, SS49, 2011<br/><br/>\n<a id=\"ref3\"></a>\n[3] R. Trivedi, T. Singh, N. Gupta. Prediction of Blast-Induced Flyrock in Opencast Mines Using ANN and ANFIS. <br/><br/>\n<a id=\"ref4\"></a>\n[4] CVB Cunningham. The Kuz-Ram fragmentation model  20 years on. In Brighton Conference Proceedings, pages 201  210, 2005 <br/><br/>\n<a id=\"ref5\"></a>\n[5] E. Hamdi, J. du Mouza. A methodology for rock mass characterization and classification to improve blast results. International Journal of Rock Mechanics and Mining Sciences. 42 Pages 177  194, 2005. <br/><br/>\n<a id=\"ref6\"></a>\n[6] W. Zhou and N. Maerz. Implementation of multivariate clustering methods for characterizing discontinuities data from scanlines and oriented boreholes. Computers and Geosciences. Vol 28 Pages 827  839, 2002.<br/><br/>\n<a id=\"ref7\"></a>\n[7] A Jain, M Murty, and P Flynn. Data Clustering : A Review. In ACM Digital Library. Vol 31, Iss 3. Pages 264  323, 1999.<br/><br/>\n<a id=\"ref8\"></a>\n[8] A Jain. Data Clustering : 50 Years Beyond K-Means. Published in 19th International conference in Pattern Recognition (ICPR) . Vol 31, Iss. 8, Pages 651  666, 2010.<br/><br/>"],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.1","nbconvert_exporter":"python","file_extension":".py"},"name":"rao_amol_mine_blastingV4","notebookId":2109564766429405,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":0}
